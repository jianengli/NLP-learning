{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 回答以下理论题目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.  What is autoencoder?\n",
    "神经网络要接受大量的输入信息, 比如输入信息是高清图片时, 输入信息量可能达到上千万, 让神经网络直接从上千万个信息源中学习是一件很吃力的工作。所以, 压缩可以提取出原图片中的最具代表性的信息, 缩减输入信息量, 再把缩减过后的信息放进神经网络学习. 这样学习就简单轻松。\n",
    "\n",
    "所以, 自编码就通过将原数据的X1压缩, 解压成X2, 然后通过对比X1，X2 ,求出预测误差, 进行反向传递, 逐步提升自编码的准确性。训练好的自编码中间这一部分就是能总结原数据的精髓。可以看出,从头到尾, 我们只用到了输入数据 X1, 并没有用到 X1对应的数据标签, 所以也可以说自编码是一种非监督学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What are the differences between greedy search and beam search?\n",
    "贪心搜索是一种来自计算机科学的算法，生成第一个词的分布以后，它将会根据你的条件语言模型挑选出最有可能的第一个词进入你的机器翻译模型中，在挑选出第一个词之后它将会继续挑选出最有可能的第二个词，然后继续挑选第三个最有可能的词，这种算法就叫做贪心搜索，但是你真正需要的是一次性挑选出整个单词序列。\n",
    "\n",
    "最基础的Beam Search 其实思路非常简单。就是在计算概率分布的时候，不要只选择概率最大的那个。 而是选择概率最大的几个，然后这几个作为备选方案 再往后看看。然后，结合其与之后的几个单词 一起计算概率。\n",
    "\n",
    "贪心搜索可以认为beam size为1时的集束搜索特例。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the intuition of attention mechanism?\n",
    "Attention model其实模拟的是人脑的注意力模型，举个例子来说，当我们观赏一幅画时，虽然我们可以看到整幅画的全貌，但是在我们深入仔细地观察时，其实眼睛聚焦的就只有很小的一块，这个时候人的大脑主要关注在这一小块图案上，也就是说这个时候人脑对整幅图的关注并不是均衡的，是有一定的权重区分的。\n",
    "\n",
    "比如翻译句子时，不需要后面词的信息，只需要句子里若干个词。在这些词里，对输入的词做单独操作与筛选。\n",
    "例如，翻译下面句子: 我喜欢游泳->I like swimming\n",
    "\n",
    "那么在翻译的时候可以这样,也就是越靠近相对应的词，我越注意，影响也就越大\n",
    "1. i = f(0.7(“我”),0.2(“喜欢”)+0.1(“游泳”))\n",
    "2. like = f(0.2(“我”),0.6(“喜欢”)+0.2(“游泳”))\n",
    "3. swimming = f(0.1(“我”),0.2(“喜欢”)+0.7(“游泳”))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. What is the disadvantage of word embeding introduced in previous lectures ?\n",
    "word embeding使用目标周围的词来描述它，但是每个词只有一个词向量，因此它不能根据不同的上下文来解决一词多义问题，例如bank这个词，向量无法区分意思是银行还是河岸。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What is the architecture of ELMo model. (A brief description is enough)\n",
    "ELMo使用两个具有不同方向（向前和向后）的两层LSTM模型，将上下文影响添加到单词向量中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Compared to RNN,  what is the advantage of Transformer ?\n",
    "1. 突破了 RNN 模型不能并行计算的限制。\n",
    "2. 自注意力可以产生更具可解释性的模型。我们可以从模型中检查注意力分布。各个注意头(attention head)可以学会执行不同的任务。\n",
    "3. Transformer可以通过self-attention让每个词和所有词进行直接交互，可建立长程联系，而RNN在建立长程联系的时候会出现bias或者梯度消失的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Why we use layer normalizaiton instead of batch normalization in Transformer ?\n",
    "Batch normalization是对一个Batch中每一个句子的同一位置的词的词向量进行normalization，而事实上每个句子同一位置的词一般都不同、意思也不一样，这样进行normalization将意义；\n",
    "\n",
    "而layer normalization是对每个词自己的词向量维度进行normalization，不存在这种问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Why we need position embedding in Transformer ?\n",
    "因为Transformer里只有attention，而attention不像RNN会有一个时序的差异，已经包含了词语顺序信息；所以Transformer在整句输入并行处理时，为了体现input的时序或位置的差异，它需要额外输入一个position信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Briefly describe what is self-attention and what is multi-head attention?\n",
    "1. self-attention：\n",
    "以一般的RNN的S2S为例子，一般的attention的Q来自Decoder（如下图中的大H），K和V来自Encoder（如下图中的小h）。self-attention就是attention的K、Q、V都来自encoder或者decoder，使得每个位置的表示都具有全局的语义信息，有利于建立长依赖关系。\n",
    "2.  multi-head attention\n",
    "MultiHead可以看成是一种ensemble方式，获取不同子空间的语义。将模型分为多个头，形成多个子空间，可以让模型去关注不同方面的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. What is the basic unit of GPT model?\n",
    "GPT使用transformer的decoder作为模型的基本单元，是一个单向模型，可以理解为单向版本的BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Briefly descibe how to use GPT in other NLP tasks?\n",
    "1. 多分类：例如其中最简单的分类任务，如对于句子的感情色彩识别问题，只涉及单个句子，结果是二分类。\n",
    "2. 文本蕴含\n",
    "3. 相似度：由于两句之间没有相互关系，则需要将两句用加入定界符按不同前后顺序连接，分别输入模型，生成不同的隐藏层数据再代入最终的全连接层。\n",
    "4. 多项选择：输入多个问题和答案组，得出概率最大的那个。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. What is masked language model in BERT ?\n",
    "Mask language model是使用Transformer来训练language model的一种模型。它类似完形填空，所有的位置信息都在，但是某些需要预测的词已经用mask替代。\n",
    "输入词有80%概率被替换成“[mask]”标签，10%几率随机采样的一个单词来作标签，10%几率不变。然后网络的目标就是预测“[mask]”位置的词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. What are the inputs of BERT ?\n",
    "BERT的输入参数是多个句子的编码向量。编码向量是token embedding、position embedding、segment embedding三部分的和。\n",
    "1. Token embedding表示当前词的embedding\n",
    "2. Segment embedding表示所在句子的index embedding\n",
    "3. Position embedding表示当前词所在位置的index embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. Briely descibe how to use BERT in other NLP tasks.\n",
    "1. 红楼梦知识提取\n",
    "2. 情报检测\n",
    "3. 文章写作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15. What are the differences between these three models: GPT, BERT, GPT2.\n",
    "\n",
    "GPT，Bert和GPT-2区别：\n",
    "\n",
    "1. 语言模型：Bert和GPT-2虽然都采用transformer，但是GPT结构为单向+Transformer；Bert使用的是transformer的encoder，即：Self Attention，是双向的语言模型；而GPT-2用的是transformer中去掉中间Encoder-Decoder Attention层的decoder，即：Marked Self Attention，是单向语言模型。\n",
    "2. 结构：Bert是pre-training + fine-tuning的结构；而GPT-2只有pre-training。\n",
    "3. 输入向量：GPT是Text + Pos embedding；GPT-2是token embedding + prosition embedding；Bert是 token embedding + position embedding + segment embedding。\n",
    "4. 参数量：Bert是3亿参数量；而GPT-2是15亿参数量。\n",
    "5. Bert引入Marked LM和Next Sentence Prediction；而GPT-2只是单纯的用单向语言模型进行训练，没引入这两个。\n",
    "6. Bert不能做生成式任务，而GPT-2可以。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
